# -*- coding: utf-8 -*-
"""LSTM_Model_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WFgOOB10ncocS-GJvo3gfTn5C5-QzCQP
"""

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

"""### Mecab 설치"""

! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

ls

cd Mecab-ko-for-Google-Colab/

ls

! bash install_mecab-ko_on_colab190912.sh

# mecab 테스트하기
from konlpy.tag import Mecab
mecab = Mecab()
text = u"""colab에서 mecab테스트 중입니다!"""
mecab.morphs(text)

"""### 필요한 라이브러리들 설치
- keras/tensorflow
- numpy, pandas, matplotlib
- sklearn
"""

!pip install tensorflow
!pip install keras

import keras
import tensorflow as tf
print(keras.__version__)
print(tf.__version__)

!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install scikit-learn

"""#### 라이브러리 import"""

import os, json, glob, sys
import pandas as pd
import numpy as np
from keras.utils import np_utils
import matplotlib.pyplot as plt
import matplotlib as mpl

import tensorflow as tf

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Flatten, Dropout, Input, Conv1D, MaxPooling1D, GlobalMaxPool1D
from keras.utils import np_utils
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split

from konlpy.tag import Mecab
mecab = Mecab()

"""#### Dataset 불러오기"""

path = "/content/drive/MyDrive/STT_/web_crawling/"
file_list = os.listdir(path)
names = [file[:-4] for file in file_list]
names

df = pd.DataFrame(columns=['topic', 'sub_topic', 'contents', 'label'])

idx = 0
topic_index = dict()
for name in names:
    temp_df = pd.read_csv(path+"/"+name+'.csv', encoding= 'cp949', names=['url', 'sub_topic', 'contents'])
    len_ = len(temp_df)
    temp_df['topic'] = [name]*len_
    temp_df['label'] = [idx]*len_
    temp_df = temp_df.loc[:, ['topic', 'sub_topic', 'contents','label']]
    df = pd.concat([df, temp_df])
    topic_index[name] = idx
    idx += 1
    del temp_df
df

# 데이터 섞기
df_shuffled=df.sample(frac=1).reset_index(drop=True)
df_shuffled.head()

# 데이터 셋 크기
len(df)

"""#### Data preprocessing"""

# 데이터 셋을 input, output구별
X = df_shuffled.loc[:, 'contents'].values
Y = df_shuffled.loc[:, 'label'].values

topic_index

unique_elements, counts_elements = np.unique(Y, return_counts=True)
print("각 레이블에 대한 빈도수:")
print(np.asarray((unique_elements, counts_elements)))

plt.bar(unique_elements, counts_elements, width=0.3, align='edge', color="blue",
        edgecolor="gray", linewidth=3, tick_label=unique_elements, log=True)

print(len(X))
print(len(Y))

# stop word 불러오기
path = '/content/drive/MyDrive/STT_/korean_stopwords.txt'
f = open(path, 'r', encoding='utf-8')
stop_words = []
while True:
    line = f.readline()
    if not line: break
    stop_words.append(line.split()[0])
f.close()

stop_words[:5]

new_X = []
delete_Y_idxs = []
for idx in range(len(X)):
  # 1. 토큰화
  temp = mecab.morphs(X[idx])
  # 2. 불용어 제거
  temp = [i for i in temp if i not in stop_words and i != '.']
  if temp:
      new_X.append(temp)
  else:
    delete_Y_idxs.append(idx)

print(len(new_X))
print(len(X))
print(len(delete_Y_idxs))

# 필요없는 부분 제거
new_Y = np.delete(Y, delete_Y_idxs)
print(len(new_X))
print(len(X))
print(len(new_Y))
print(len(Y))

new_X[4]

# X 정수 encoding하기
max_word = 10000
max_len = 1000 #max_len = 500

tok = Tokenizer(num_words = max_word)
tok.fit_on_texts(new_X)
sequences = tok.texts_to_sequences(new_X)
print(len(tok.word_index))

"""각 기사의 단어 수가 제각각 다르므로 이를 동일하게 맞춰야 한다.
이때는 다음과 같이 데이터 전처리 함수 sequence()를 이용한다.
"""

sequences_matrix  = sequence.pad_sequences(sequences, maxlen=max_len)
print(sequences_matrix)
print(len(sequences_matrix[0]))

type(sequences_matrix)

type(new_Y)

print(len(sequences_matrix))

# new_Y 데이터에 원-핫 인코딩 처리를 하여 데이터 전처리 과정을 마친다.
nb_classes = len(set(new_Y))
print(nb_classes)
y = np_utils.to_categorical(new_Y, nb_classes)

print(y[100])

X_train, X_test, y_train, y_test = train_test_split(sequences_matrix, y, test_size=0.2)

print(X_train.shape)
print(y_train.shape)

print(X_test.shape)
print(y_test.shape)

"""##### training set X : 308024, 1000
##### training set Y : 308024, 91

##### training set X : 77006, 1000
##### training set Y : 77006, 91

### Model 설계 및 훈련
"""

model = Sequential()

model.add(Embedding(max_word, 64, input_length=max_len))
model.add(LSTM(60, return_sequences=True))
model.add(GlobalMaxPool1D())
model.add(Dropout(0.2))
model.add(Dense(50, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model_dir = '/content/drive/MyDrive/STT_/model'
if not os.path.exists(model_dir):
    os.mkdir(model_dir)
model_path = model_dir + "/Naver_dict_lstm.h5"

model_path

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)
mc = ModelCheckpoint(filepath=model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.summary()

history = model.fit(X_train, y_train, batch_size=256, epochs=50, callbacks=[es, mc], validation_split=0.2)

print("정확도 : %.4f" % (model.evaluate(X_test, y_test)[1]))

y_vloss = hist.history['val_loss']
y_loss = hist.history['loss']

x_len = np.arange(len(y_loss))

plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')
plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
plt.grid()
plt.show()

y_vloss = hist.history['val_acc']
y_loss = hist.history['acc']

x_len = np.arange(len(y_loss))

plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')
plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
plt.grid()
plt.show()

str = ['''이번 시간에는 문화의 정의와 구분 방식, 문화의 유동성과 통번역과의 관계를 학습하겠습니다.
또 통번역의 역사적인 발전과 중요성을 세계사와 국내 역사의 관점에서 살펴본 다음,
통번역과 국가경쟁력, 또 통번역과 개개인의 경쟁력 간의 관계를 알아보겠습니다.
그럼 우리가 함께 살펴볼 첫 번째 개념이 문화입니다. 이 어휘는 실생활에서 우리가 너무 흔하게 접하기 때문에 의미를 크게 깊이 생각하지 않고 사용하는 어휘 가운데 하나입니다.
우리가 대체로 한국문화, 미국문화, 중국문화와 같이 특정 집단 사람들이 공유하는 특징적인 삶의 방식이나 지식을 가리키는 데 사용하죠.
여기에는 언어, 정치, 경제, 종교, 예술 같은 것은 물론이고 음식이나 혼례방식, 장례방식 같은 사회적인 관습도 포함됩니다.
학자들은 가시적으로 볼 수 없는 정신이나 가치를 문화라고 부르는 데 비해서 건축이나 예술, 테크놀로지 같은 것은 문명으로 구분하는 경향이 있습니다.
그러면 문화라는 단어의 어원은 어디에 있을까요? 문화를 뜻하는 영어 어휘는 ‘culture’인데 이 어휘의 어원이 라틴어의 ‘colere’입니다. 즉, 땅을 돌보고 길러낸다는 의미죠.
이렇게 볼 때 문화라는 것은 인간이 발 디디고 생활하는 환경을 좀 더 좋게 바꿔나가는 모든 과정 또는 그것의 결과물을 가리킨다고 생각할 수 있겠습니다.
문화의 특징은 크게 두 가지인데, 첫 번째로는 특정 집단과 다른 집단을 구분 지어주는 역할을 한다는 것.
그리고 두 번째로는 특정 집단의 가치와 사고방식을 집단적으로 학습하는 과정. 곧 사회화를 통해서 구성원들이 공유하게 된다는 점입니다.
우리가 문화를 다양한 방식으로 구분할 수 있는데요. 고대문명의 발상지를 기준으로 할 때는 이집트문명, 갠지스문명, 메소포타미아문명 이렇게 구분하기도 하고요.
아시아문화, 유럽문화와 같이 문화가 분포하는 지리적인 특성이나 인종에 따라 구분하기도 합니다.
또 향유주체, 즉 문화를 누리는 사람이 누군가에 따라서 대중문화냐, 고급문화냐를 구분하거나 조직문화냐, 개인 문화냐와 같이 다양한 관점에 따라 분류하기도 합니다.
여기에서 주목할 것은 문화의 속성이 끊임없는 변화와 유동성이라는 거죠.
학자들은 문화가 단 한 번도 고정되어 있거나 정형화된 채로 머물러 있지 않고 본질적으로 끊임없이 변화한다고 강조합니다. 
''']
sequences_6m = tok.texts_to_sequences(str)
print(sequences_6m)
sequences_matrix_6m = sequence.pad_sequences(sequences_6m, maxlen=500)
model.predict_classes(sequences_matrix_6m)