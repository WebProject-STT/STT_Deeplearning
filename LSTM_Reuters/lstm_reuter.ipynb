{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LSTM을 이용한 로이터 뉴스 카테고리 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로이터 뉴스 데이터셋 불러오기\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\msi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "c:\\users\\msi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# 불러온 데이터를 학습셋과 테스트셋으로 나누기\n",
    "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words=1000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_split 인자를 통해 20%를 테스트셋으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 카테고리\n",
      "8982 학습용 뉴스 기사\n",
      "2246 테스트용 뉴스 기사\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인 후 출력\n",
    "import numpy as np\n",
    "category = np.max(Y_train) + 1\n",
    "print(category, '카테고리')\n",
    "print(len(X_train), '학습용 뉴스 기사')\n",
    "print(len(X_test), '테스트용 뉴스 기사')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "단어를 그대로 사용하지 않고 숫자로 변환한 다음 힉습한다.\n",
    "여기서는 데이터 안에서 해당 단어가 몇 번이나 나타나는지 세어 빈도에 따라 번호를 붙였습니다.\n",
    "기사 안의 모든 단어를 다 사용하는 것은 비효율적이므로 빈도가 높은 단어만 불러와 사용한다.\n",
    "이때 사용하는 인자가 바로 테스트셋과 학습셋으로 나눌 때 함께 적용했던 num_word=1000이다.\n",
    "빈도가 1~1,000에 해당하는 단어만 선택해서 불러온다.\n",
    "\"\"\"\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 레이블에 대한 빈도수:\n",
      "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "    42   43   44   45]\n",
      " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
      "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
      "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
      "    13   21   12   18]]\n"
     ]
    }
   ],
   "source": [
    "# 3, 4가 가장 많은 레이블을 차지하는 것을 확인할 수 있다.\n",
    "unique_elements, counts_elements = np.unique(Y_train, return_counts=True)\n",
    "print(\"각 레이블에 대한 빈도수:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "# label_cnt=dict(zip(unique_elements, counts_elements))\n",
    "# 아래의 출력 결과가 보기 불편하여 병렬로 보고싶다면 위의 label_cnt를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "각 기사의 단어 수가 제각각 다르므로 이를 동일하게 맞춰야 한다.\n",
    "이때는 다음과 같이 데이터 전처리 함수 sequence()를 이용한다.\n",
    "maxlen=100은 단어 수를 100개로 맞춘다.\n",
    "만일 입력된 기사의 단어 수가 100보다 크면 100개째 단어만 선택하고 나머지는 버립니다.\n",
    "100에서 모자랄 때는 모자라는 부분을 모두 0으로 채웁니다.\n",
    "\"\"\"\n",
    "from keras.preprocessing import sequence\n",
    "  \n",
    "# 데이터 전처리\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  y 데이터에 원-핫 인코딩 처리를 하여 데이터 전처리 과정을 마친다.\n",
    "# 데이터 전처리\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding 층은 데이터 전처리 과정을 통해 입력된 값을 받아 다음 층이 알아들을 수 있는 형태로 변환하는 역할\n",
    "* Embedding('불러온 단어의 총 개수', '기사당 단어 수') 형식\n",
    "<br>\n",
    "<br>\n",
    "* LSTM은 앞서 설명했듯이 RNN에서 기억 값에 대한 가중치를 제어하며, LSTM(기사당 단어 수, 기타 옵션)의 형식으로 적용\n",
    "* LSTM의 활성화 함수로는 tanh를 사용\n",
    "<br>\n",
    "<br>\n",
    "* 46개의 카테고리를 분류해야하므로, 출력층에서는 46개의 뉴런을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 100))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 학습을 조기 종료(Early Stopping)한다.\n",
    "* ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다중 클래스 분류(Multi-Class Classification) 문제이므로 손실 함수로는 categorical_crossentropy를 사용한다.\n",
    "* categorical_crossentropy는 모델의 예측값과 실제값에 대해서 두 확률 분포 사이의 거리를 최소화하도록 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델을 학습합니다. validation_data로 X_test와 y_test를 사용합니다.\n",
    "* val_loss가 줄어들다가 증가하는 상황이 오면 과적합(overfitting)으로 판단하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 2.1233 - acc: 0.4604\n",
      "Epoch 00001: val_acc improved from -inf to 0.48798, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 100ms/step - loss: 2.1227 - acc: 0.4606 - val_loss: 2.0255 - val_acc: 0.4880\n",
      "Epoch 2/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.9029 - acc: 0.5040\n",
      "Epoch 00002: val_acc improved from 0.48798 to 0.51024, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 95ms/step - loss: 1.9022 - acc: 0.5039 - val_loss: 1.8211 - val_acc: 0.5102\n",
      "Epoch 3/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.7326 - acc: 0.5484\n",
      "Epoch 00003: val_acc improved from 0.51024 to 0.54987, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 96ms/step - loss: 1.7332 - acc: 0.5485 - val_loss: 1.7464 - val_acc: 0.5499\n",
      "Epoch 4/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.6707 - acc: 0.5708\n",
      "Epoch 00004: val_acc improved from 0.54987 to 0.57079, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 97ms/step - loss: 1.6690 - acc: 0.5710 - val_loss: 1.6957 - val_acc: 0.5708\n",
      "Epoch 5/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.5906 - acc: 0.5915\n",
      "Epoch 00005: val_acc did not improve from 0.57079\n",
      "71/71 [==============================] - 7s 95ms/step - loss: 1.5905 - acc: 0.5917 - val_loss: 1.6728 - val_acc: 0.5588\n",
      "Epoch 6/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4775 - acc: 0.6254- ETA: 2s - los\n",
      "Epoch 00006: val_acc improved from 0.57079 to 0.61665, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 97ms/step - loss: 1.4796 - acc: 0.6248 - val_loss: 1.5139 - val_acc: 0.6167\n",
      "Epoch 7/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.3846 - acc: 0.6443\n",
      "Epoch 00007: val_acc improved from 0.61665 to 0.63224, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 97ms/step - loss: 1.3850 - acc: 0.6444 - val_loss: 1.4643 - val_acc: 0.6322\n",
      "Epoch 8/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.4053 - acc: 0.6419\n",
      "Epoch 00008: val_acc improved from 0.63224 to 0.63669, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 97ms/step - loss: 1.4058 - acc: 0.6418 - val_loss: 1.4306 - val_acc: 0.6367\n",
      "Epoch 9/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2814 - acc: 0.6698\n",
      "Epoch 00009: val_acc did not improve from 0.63669\n",
      "71/71 [==============================] - 7s 98ms/step - loss: 1.2812 - acc: 0.6698 - val_loss: 1.4348 - val_acc: 0.6278\n",
      "Epoch 10/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.2273 - acc: 0.6799\n",
      "Epoch 00010: val_acc improved from 0.63669 to 0.65227, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 98ms/step - loss: 1.2269 - acc: 0.6800 - val_loss: 1.3778 - val_acc: 0.6523\n",
      "Epoch 11/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.1678 - acc: 0.6991\n",
      "Epoch 00011: val_acc improved from 0.65227 to 0.65806, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 101ms/step - loss: 1.1680 - acc: 0.6988 - val_loss: 1.3317 - val_acc: 0.6581\n",
      "Epoch 12/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.1127 - acc: 0.7165\n",
      "Epoch 00012: val_acc improved from 0.65806 to 0.67097, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 102ms/step - loss: 1.1135 - acc: 0.7164 - val_loss: 1.2896 - val_acc: 0.6710\n",
      "Epoch 13/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.0629 - acc: 0.7247\n",
      "Epoch 00013: val_acc improved from 0.67097 to 0.68077, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 102ms/step - loss: 1.0621 - acc: 0.7249 - val_loss: 1.2839 - val_acc: 0.6808\n",
      "Epoch 14/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 1.0167 - acc: 0.7400\n",
      "Epoch 00014: val_acc improved from 0.68077 to 0.68789, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 101ms/step - loss: 1.0161 - acc: 0.7400 - val_loss: 1.2571 - val_acc: 0.6879\n",
      "Epoch 15/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.9812 - acc: 0.7499\n",
      "Epoch 00015: val_acc did not improve from 0.68789\n",
      "71/71 [==============================] - 7s 103ms/step - loss: 0.9802 - acc: 0.7501 - val_loss: 1.2750 - val_acc: 0.6817\n",
      "Epoch 16/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.9613 - acc: 0.7558\n",
      "Epoch 00016: val_acc did not improve from 0.68789\n",
      "71/71 [==============================] - 7s 105ms/step - loss: 0.9607 - acc: 0.7560 - val_loss: 1.2692 - val_acc: 0.6812\n",
      "Epoch 17/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.9109 - acc: 0.7673\n",
      "Epoch 00017: val_acc improved from 0.68789 to 0.69101, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 105ms/step - loss: 0.9111 - acc: 0.7673 - val_loss: 1.2666 - val_acc: 0.6910\n",
      "Epoch 18/30\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.8797 - acc: 0.7743\n",
      "Epoch 00018: val_acc improved from 0.69101 to 0.69145, saving model to best_model.h5\n",
      "71/71 [==============================] - 7s 104ms/step - loss: 0.8786 - acc: 0.7745 - val_loss: 1.2720 - val_acc: 0.6915\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 12ms/step - loss: 1.2720 - acc: 0.6915\n",
      "\n",
      " Test Accuracy: 0.6915\n"
     ]
    }
   ],
   "source": [
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋의 오차\n",
    "y_vloss = history.history['val_loss']\n",
    "  \n",
    "# 학습셋의 오차\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ff59bc09e8>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 표현\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
